#!/usr/bin/env python3
"""
OSINT: Social Media Finder (Upgraded)
- Improved carrier detection (phonenumbers + optional Numverify API)
- Broader social search across many platforms (Instagram, Pinterest, TikTok, Reddit, Snapchat, Telegram, WhatsApp, Facebook, LinkedIn, Twitter)
- Multi-search-engine scraping (Bing, DuckDuckGo, Google)
- Extra detail extraction (candidate names, emails, snippets)
- CLI, JSON/CSV export, optional proxy support, threading
- Public OSINT only ‚Äî do not bypass login/CAPTCHA or private APIs without consent.

Dependencies:
  pip install phonenumbers requests beautifulsoup4
Optional for more accurate carrier lookup:
  Sign up at https://numverify.com (or similar) and supply --numverify API_KEY
"""

from concurrent.futures import ThreadPoolExecutor, as_completed
import requests
from bs4 import BeautifulSoup
import json
import csv
import time
import re
from urllib.parse import quote_plus, urljoin
import phonenumbers
from phonenumbers import carrier, geocoder
import argparse
import random
import logging
import sys
import os
from typing import List, Dict, Any, Optional

# -------------------------
# Logging / Globals
# -------------------------
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
log = logging.getLogger("osint_upgraded")

BASE_HEADERS = {
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Connection": "keep-alive",
}

USER_AGENTS = [
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 Chrome/122.0",
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:115.0) Gecko/20100101 Firefox/115.0",
    "Mozilla/5.0 (Linux; Android 12; Pixel 5) AppleWebKit/537.36 Mobile Safari/537.36",
    "Mozilla/5.0 (Macintosh; Intel Mac OS X 13_0) AppleWebKit/605.1.15 Safari/605.1.15",
]

DEFAULT_DELAY = 1.3

# -------------------------
# Banner
# -------------------------
def banner():
    print(r"""
     ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó   ‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó‚ñà‚ñà‚ïó    ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïó ‚ñà‚ñà‚ñà‚ñà‚ñà
    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ïó  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïë ‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ïê‚ïê‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïó
    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë ‚ñà‚ïó ‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë
    ‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïî‚ïê‚ñà‚ñà‚ïó ‚ñà‚ñà‚ïë   ‚ñà‚ñà‚ïë‚ïö‚ïê‚ïê‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ïó‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë‚ñà‚ñà‚ïî‚ïê‚ïê‚ñà‚ñà‚ïë
    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë ‚ïö‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë    ‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïó‚ïö‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ïö‚ñà‚ñà‚ñà‚ïî‚ñà‚ñà‚ñà‚ïî‚ïù‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë‚ñà‚ñà‚ïë  ‚ñà‚ñà‚ïë
    ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù    ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù ‚ïö‚ïê‚ïê‚ïù‚ïö‚ïê‚ïê‚ïù ‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù‚ïö‚ïê‚ïù  ‚ïö‚ïê‚ïù

    OPEN SOURCE INTELLIGENCE TOOL (OSINT) v2.3 üíÄ
     
    üßëüèª‚Äçüíª Created by :-  ùî∏‚ÑïùïÄùïä‚Ñç ùïÇùïåùïä‚Ñçùïéùî∏‚Ñçùî∏ üí´
    üåê Website    :-  Anish-kushwaha.b12sites.com
    üìß E-mail     :-  Anish_Kushwaha@proton.me

    ‚öö‚í∂‚öö

    """)

# -------------------------
# Network helper (UA rotation, optional proxies, retry/backoff)
# -------------------------
def load_proxies_from_file(path: str) -> List[str]:
    proxies = []
    try:
        with open(path, "r", encoding="utf-8") as fh:
            for line in fh:
                line = line.strip()
                if line:
                    proxies.append(line)
    except Exception as e:
        log.warning("Could not read proxies file: %s", e)
    return proxies

def request_get(url: str,
                headers: Optional[dict] = None,
                proxies: Optional[List[str]] = None,
                timeout: int = 12,
                max_retries: int = 3,
                backoff: float = 1.5) -> Optional[requests.Response]:
    session = requests.Session()
    attempt = 0
    while attempt < max_retries:
        ua = random.choice(USER_AGENTS)
        req_headers = dict(BASE_HEADERS)
        req_headers["User-Agent"] = ua
        if headers:
            req_headers.update(headers)
        proxy_dict = None
        if proxies:
            p = random.choice(proxies)
            proxy_dict = {"http": p, "https": p}
        try:
            resp = session.get(url, headers=req_headers, proxies=proxy_dict, timeout=timeout)
            # treat 200/301/302 as usable
            if resp.status_code in (200, 301, 302):
                return resp
            if resp.status_code in (429, 403, 503):
                wait = backoff * (2 ** attempt)
                log.debug("Got %d from %s ‚Äî sleeping %.1fs", resp.status_code, url, wait)
                time.sleep(wait)
            else:
                return resp
        except requests.RequestException as e:
            wait = backoff * (2 ** attempt)
            log.debug("Request exception %s (attempt %d) for %s ‚Äî sleeping %.1fs", e, attempt+1, url, wait)
            time.sleep(wait)
        attempt += 1
    log.debug("Max retries reached for %s", url)
    return None

# -------------------------
# Helper: search multiple engines
# -------------------------
def search_engine_query_all(pattern: str, proxies: Optional[List[str]] = None) -> List[str]:
    """
    Run queries across Bing and DuckDuckGo (and Google optionally).
    Return list of candidate URLs (strings). This broadens coverage.
    """
    results = []
    engines = {
        "bing": f"https://www.bing.com/search?q={quote_plus(pattern)}",
        "ddg": f"https://duckduckgo.com/html/?q={quote_plus(pattern)}",
        # Google often blocks scraping; keep it optional and last-resort
        "google": f"https://www.google.com/search?q={quote_plus(pattern)}"
    }
    for name, url in engines.items():
        try:
            r = request_get(url, proxies=proxies)
            if not r:
                continue
            soup = BeautifulSoup(r.text, 'html.parser')
            # collect visible anchor hrefs
            anchors = soup.find_all('a', href=True)
            for a in anchors:
                href = a.get('href')
                # filter out internal search redirects
                if href and href.startswith('/'):
                    continue
                if href and (href.startswith('http://') or href.startswith('https://')):
                    results.append(href.split('?')[0])
            time.sleep(0.8)
        except Exception as e:
            log.debug("Engine %s search failed: %s", name, e)
    # deduplicate keeping order
    seen = set()
    out = []
    for u in results:
        if u not in seen:
            seen.add(u)
            out.append(u)
    return out

# -------------------------
# Profile validators & extractors
# -------------------------
def http_probe_ok(url: str, proxies: Optional[List[str]] = None) -> bool:
    r = request_get(url, proxies=proxies)
    if not r:
        return False
    # check typical profile page existence heuristics
    if r.status_code == 200 and len(r.text) > 200:
        return True
    return False

def extract_page_snippets(url: str, proxies: Optional[List[str]] = None) -> Dict[str, Any]:
    """
    Retrieve page and return short snippet info (title, meta description, emails found, name-like tokens).
    """
    out = {"title": None, "description": None, "emails": [], "text_snippet": None}
    r = request_get(url, proxies=proxies)
    if not r:
        return out
    try:
        soup = BeautifulSoup(r.text, 'html.parser')
        title = soup.title.string.strip() if soup.title and soup.title.string else None
        desc_tag = soup.find('meta', attrs={'name':'description'}) or soup.find('meta', attrs={'property':'og:description'})
        desc = desc_tag.get('content').strip() if desc_tag and desc_tag.get('content') else None
        text = soup.get_text(separator=' ', strip=True)
        # find emails
        emails = re.findall(r'[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}', r.text)
        out['title'] = title
        out['description'] = desc
        out['emails'] = list(set(emails))
        out['text_snippet'] = text[:400] if text else None
    except Exception:
        pass
    return out

# -------------------------
# Search functions (platform-specific)
# -------------------------
def search_whatsapp(phone_number: str, proxies: Optional[List[str]] = None, delay: float = DEFAULT_DELAY) -> List[Dict[str,Any]]:
    results=[]
    try:
        clean = re.sub(r'[^\d+]', '', phone_number)
        url = f"https://wa.me/{clean}"
        results.append({'platform':'WhatsApp','method':'Direct Link','found':True,'url':url,'details':'WhatsApp contact link (direct)'})
        time.sleep(delay)
    except Exception as e:
        results.append({'platform':'WhatsApp','found':False,'error':str(e)})
    return results

def search_facebook(phone_number: str, proxies: Optional[List[str]] = None, delay: float = DEFAULT_DELAY) -> List[Dict[str,Any]]:
    results=[]
    try:
        # Account recovery
        url_recover = f"https://www.facebook.com/login/identify/?ctx=recover&phone={quote_plus(phone_number)}"
        r = request_get(url_recover, proxies=proxies)
        if r and "find your account" in r.text.lower():
            results.append({'platform':'Facebook','method':'Account Recovery','found':True,'url':url_recover,'details':'Account appears in recovery flow'})
        # public search
        url_public = f"https://www.facebook.com/public?query={quote_plus(phone_number)}"
        r2 = request_get(url_public, proxies=proxies)
        if r2:
            soup = BeautifulSoup(r2.text, 'html.parser')
            anchors = soup.find_all('a', href=True)
            seen=set()
            for a in anchors:
                href=a['href'].split('?')[0]
                if 'facebook.com' in href and href not in seen:
                    seen.add(href)
                    results.append({'platform':'Facebook','method':'Public Search','found':True,'url':href,'details':'Potential profile found in public search'})
        time.sleep(delay)
    except Exception as e:
        results.append({'platform':'Facebook','found':False,'error':str(e)})
    return results

def search_instagram(phone_number: str, proxies: Optional[List[str]] = None, delay: float = DEFAULT_DELAY) -> List[Dict[str,Any]]:
    """
    Improved Instagram logic: look for instagram.com/<candidate> via search engines,
    then probe the profile page to verify existence and extract bio/title.
    """
    results=[]
    try:
        patterns = [
            f'site:instagram.com "{phone_number}"',
            f'"instagram.com" "{phone_number}"',
            f'inurl:instagram.com "{phone_number}"',
            f'"@{phone_number}" instagram'
        ]
        candidate_usernames=set()
        for p in patterns:
            urls = search_engine_query_all(p, proxies=proxies)
            for u in urls:
                m = re.search(r'https?://(?:www\.)?instagram\.com/([A-Za-z0-9._-]+)/?', u)
                if m:
                    uname = m.group(1).strip('/')
                    candidate_usernames.add(uname)
            time.sleep(0.6)
        # Also try to search for plain phone number as token and extract IG URLs
        urls = search_engine_query_all(phone_number, proxies=proxies)
        for u in urls:
            m = re.search(r'https?://(?:www\.)?instagram\.com/([A-Za-z0-9._-]+)/?', u)
            if m:
                candidate_usernames.add(m.group(1))
        # probe candidates
        for uname in list(candidate_usernames)[:20]:
            profile = f"https://www.instagram.com/{uname}/"
            ok = http_probe_ok(profile, proxies=proxies)
            if ok:
                snippet = extract_page_snippets(profile, proxies=proxies)
                results.append({'platform':'Instagram','method':'Profile Probe','found':True,'url':profile,'username':uname,'details': snippet})
            time.sleep(delay)
    except Exception as e:
        results.append({'platform':'Instagram','found':False,'error':str(e)})
    return results

def search_linkedin(phone_number: str, proxies: Optional[List[str]] = None, delay: float = DEFAULT_DELAY) -> List[Dict[str,Any]]:
    results=[]
    try:
        patterns = [
            f'site:linkedin.com "{phone_number}"',
            f'phone {phone_number} linkedin',
            f'"linkedin.com/in" "{phone_number}"'
        ]
        seen=set()
        for p in patterns:
            urls = search_engine_query_all(p, proxies=proxies)
            for u in urls:
                if 'linkedin.com/in' in u:
                    u_clean = u.split('?')[0]
                    if u_clean not in seen:
                        seen.add(u_clean)
                        results.append({'platform':'LinkedIn','method':'Search','found':True,'url':u_clean,'details':'Potential LinkedIn profile'})
            time.sleep(delay)
    except Exception as e:
        results.append({'platform':'LinkedIn','found':False,'error':str(e)})
    return results

def search_twitter(phone_number: str, proxies: Optional[List[str]] = None, delay: float = DEFAULT_DELAY) -> List[Dict[str,Any]]:
    results=[]
    try:
        patterns = [
            f'site:twitter.com "{phone_number}"',
            f'phone {phone_number} twitter',
            phone_number
        ]
        seen=set()
        for p in patterns:
            urls = search_engine_query_all(p, proxies=proxies)
            for u in urls:
                m = re.search(r'https?://(?:www\.)?twitter\.com/([A-Za-z0-9_]+)/?', u)
                if m:
                    profile = f"https://twitter.com/{m.group(1)}"
                    if profile not in seen:
                        seen.add(profile)
                        results.append({'platform':'Twitter','method':'Search','found':True,'url':profile,'username':m.group(1)})
            time.sleep(delay)
    except Exception as e:
        results.append({'platform':'Twitter','found':False,'error':str(e)})
    return results

def search_github(phone_number: str, proxies: Optional[List[str]] = None, delay: float = DEFAULT_DELAY) -> List[Dict[str,Any]]:
    results=[]
    try:
        patterns=[f'site:github.com "{phone_number}"', phone_number]
        seen=set()
        for p in patterns:
            urls = search_engine_query_all(p, proxies=proxies)
            for u in urls:
                m = re.search(r'https?://github\.com/([A-Za-z0-9\-_]+)/?', u)
                if m:
                    profile=f"https://github.com/{m.group(1)}"
                    if profile not in seen:
                        seen.add(profile)
                        results.append({'platform':'GitHub','method':'Search','found':True,'url':profile,'username':m.group(1)})
            time.sleep(delay)
    except Exception as e:
        results.append({'platform':'GitHub','found':False,'error':str(e)})
    return results

def search_telegram(phone_number: str, proxies: Optional[List[str]] = None, delay: float = DEFAULT_DELAY) -> List[Dict[str,Any]]:
    results=[]
    try:
        clean = re.sub(r'[^\d+]', '', phone_number)
        patterns=[f'site:t.me "{clean}"', f'telegram {clean}', clean]
        usernames=set()
        for p in patterns:
            urls = search_engine_query_all(p, proxies=proxies)
            for u in urls:
                m = re.search(r'https?://t\.me/([A-Za-z0-9_]+)/?', u)
                if m:
                    usernames.add(m.group(1))
            time.sleep(delay)
        for uname in usernames:
            profile=f"https://t.me/{uname}"
            results.append({'platform':'Telegram','method':'Search','found':True,'url':profile,'username':uname})
    except Exception as e:
        results.append({'platform':'Telegram','found':False,'error':str(e)})
    return results

def search_reddit(phone_number: str, proxies: Optional[List[str]] = None, delay: float = DEFAULT_DELAY) -> List[Dict[str,Any]]:
    results=[]
    try:
        patterns=[f'site:reddit.com "{phone_number}"', phone_number]
        seen=set()
        for p in patterns:
            urls = search_engine_query_all(p, proxies=proxies)
            for u in urls:
                if 'reddit.com' in u and u not in seen:
                    seen.add(u)
                    results.append({'platform':'Reddit','method':'Search','found':True,'url':u})
            time.sleep(delay)
    except Exception as e:
        results.append({'platform':'Reddit','found':False,'error':str(e)})
    return results

def search_tiktok(phone_number: str, proxies: Optional[List[str]] = None, delay: float = DEFAULT_DELAY) -> List[Dict[str,Any]]:
    results=[]
    try:
        patterns=[f'site:tiktok.com "{phone_number}"', phone_number]
        seen=set()
        for p in patterns:
            urls = search_engine_query_all(p, proxies=proxies)
            for u in urls:
                if 'tiktok.com' in u and u not in seen:
                    seen.add(u)
                    results.append({'platform':'TikTok','method':'Search','found':True,'url':u})
            time.sleep(delay)
    except Exception as e:
        results.append({'platform':'TikTok','found':False,'error':str(e)})
    return results

def search_snapchat(phone_number: str, proxies: Optional[List[str]] = None, delay: float = DEFAULT_DELAY) -> List[Dict[str,Any]]:
    results=[]
    try:
        # snapchat doesn't expose public profile pages easily; rely on search results
        urls = search_engine_query_all(f'site:snapchat.com "{phone_number}"', proxies=proxies)
        for u in urls:
            results.append({'platform':'Snapchat','method':'Search','found':True,'url':u})
        time.sleep(delay)
    except Exception as e:
        results.append({'platform':'Snapchat','found':False,'error':str(e)})
    return results

def search_phone_directories(phone_number: str, proxies: Optional[List[str]] = None, delay: float = DEFAULT_DELAY) -> List[Dict[str,Any]]:
    results=[]
    try:
        patterns=[f'{phone_number} "truecaller"', f'{phone_number} "whitepages"', f'{phone_number} "sync.me"', phone_number]
        seen=set()
        for p in patterns:
            urls = search_engine_query_all(p, proxies=proxies)
            for u in urls:
                if any(x in u.lower() for x in ['truecaller','whitepages','sync.me','callapp','yellowpages','reverse']) and u not in seen:
                    seen.add(u)
                    snippet = extract_page_snippets(u, proxies=proxies)
                    results.append({'platform':'Phone Directory','method':'Search','found':True,'url':u,'details':snippet})
            time.sleep(delay)
    except Exception as e:
        results.append({'platform':'Phone Directory','found':False,'error':str(e)})
    return results

# -------------------------
# Aggregator orchestration
# -------------------------
def comprehensive_social_search(phone_number: str,
                                proxies: Optional[List[str]] = None,
                                delay: float = DEFAULT_DELAY,
                                max_threads: int = 8) -> List[Dict[str,Any]]:
    search_functions = [
        ('WhatsApp', search_whatsapp),
        ('Facebook', search_facebook),
        ('Instagram', search_instagram),
        ('LinkedIn', search_linkedin),
        ('Twitter', search_twitter),
        ('Telegram', search_telegram),
        ('GitHub', search_github),
        ('Reddit', search_reddit),
        ('TikTok', search_tiktok),
        ('Snapchat', search_snapchat),
        ('Phone Directories', search_phone_directories)
    ]
    log.info("Starting social searches with %d threads...", max_threads)
    all_results=[]
    with ThreadPoolExecutor(max_workers=max_threads) as executor:
        futures = {executor.submit(func, phone_number, proxies, delay): name for name, func in search_functions}
        for fut in as_completed(futures):
            name = futures[fut]
            try:
                res = fut.result(timeout=90)
                if isinstance(res, list):
                    all_results.extend(res)
                    found = [r for r in res if r.get('found')]
                    if found:
                        for r in found:
                            log.info("FOUND on %s: %s", r.get('platform'), r.get('url'))
                    else:
                        log.info("No public result found on %s", name)
                else:
                    all_results.append({'platform':name,'found':False,'error':'Bad format result'})
            except Exception as e:
                log.warning("Search %s failed: %s", name, e)
    return all_results

# -------------------------
# Carrier enrichment via optional external API (Numverify)
# -------------------------
def lookup_carrier_numverify(phone_number: str, api_key: str) -> Optional[Dict[str,Any]]:
    """
    Use numverify API to get carrier info. (Requires key)
    Example response: https://numverify.com/documentation
    """
    try:
        url = f"http://apilayer.net/api/validate?access_key={api_key}&number={quote_plus(phone_number)}"
        r = request_get(url)
        if r and r.status_code == 200:
            data = r.json()
            # return subset
            return {
                'valid': data.get('valid'),
                'number': data.get('international_format') or data.get('number'),
                'local_format': data.get('local_format'),
                'country_code': data.get('country_code'),
                'country_name': data.get('country_name'),
                'carrier': data.get('carrier'),
                'line_type': data.get('line_type')
            }
    except Exception as e:
        log.debug("Numverify lookup failed: %s", e)
    return None

# -------------------------
# Phone parsing & display
# -------------------------
def parse_phone_number(phone_input: str, country_code: Optional[str] = None) -> Dict[str,Any]:
    try:
        parsed = phonenumbers.parse(phone_input, country_code) if country_code else phonenumbers.parse(phone_input)
        if phonenumbers.is_valid_number(parsed):
            return {
                'e164': phonenumbers.format_number(parsed, phonenumbers.PhoneNumberFormat.E164),
                'national': phonenumbers.format_number(parsed, phonenumbers.PhoneNumberFormat.NATIONAL),
                'international': phonenumbers.format_number(parsed, phonenumbers.PhoneNumberFormat.INTERNATIONAL),
                'country_code': parsed.country_code,
                'carrier': carrier.name_for_number(parsed, "en"),
                'region': geocoder.description_for_number(parsed, "en"),
                'valid': True
            }
        else:
            return {'valid': False, 'error': 'Invalid phone number'}
    except Exception as e:
        return {'valid': False, 'error': str(e)}

def display_results(all_results: List[Dict[str,Any]], phone_info: Dict[str,Any], external_carrier: Optional[Dict[str,Any]] = None):
    print("\n" + "="*70)
    print("üéØ SOCIAL MEDIA OSINT REPORT")
    print("="*70)
    print(f"\nüìû TARGET: {phone_info.get('international', phone_info.get('e164', 'Unknown'))}")
    print(f"üìç REGION (phonenumbers): {phone_info.get('region', 'Unknown')}")
    print(f"üì° CARRIER (phonenumbers): {phone_info.get('carrier', 'Unknown')}")
    if external_carrier:
        print(f"üì° CARRIER (external API): {external_carrier.get('carrier')}  |  line_type: {external_carrier.get('line_type')}")
        if external_carrier.get('carrier') and external_carrier.get('carrier').lower() not in phone_info.get('carrier','').lower():
            print("   ‚ö†Ô∏è NOTE: Carrier differs between phonenumbers DB and external lookup ‚Äî use external result for current network if you trust it.")
    # aggregate successful
    successful = [r for r in all_results if r.get('found')]
    # collect extra details: emails, names
    emails=set()
    names=set()
    snippets=[]
    for r in successful:
        details = r.get('details')
        if isinstance(details, dict):
            for e in details.get('emails', []):
                emails.add(e)
            t = details.get('title')
            if t:
                names.add(t)
            if details.get('text_snippet'):
                snippets.append(details.get('text_snippet')[:200])
    print(f"\nüìä SEARCH SUMMARY:")
    print(f"   ‚úÖ Accounts Found: {len(successful)}")
    # Count distinct platforms
    platforms = set([r.get('platform') for r in all_results if r.get('platform')])
    print(f"   üîç Platforms Checked: {len(platforms)}")
    if successful:
        print("\nüéâ SOCIAL MEDIA ACCOUNTS FOUND:")
        print("-" * 50)
        for r in successful:
            print(f"\nüì± {r.get('platform')}:")
            print(f"   üîó {r.get('url')}")
            if r.get('username'):
                print(f"   üë§ Username: {r.get('username')}")
            if isinstance(r.get('details'), dict):
                if r['details'].get('title'):
                    print(f"   üè∑ Title/Bio: {r['details'].get('title')}")
                if r['details'].get('description'):
                    print(f"   üìù Desc: {r['details'].get('description')[:120]}")
            if r.get('details') and not isinstance(r.get('details'), dict):
                print(f"   üìù Details: {r.get('details')}")
    else:
        print("\n‚ùå No public accounts found linked to this number (with current heuristics).")
    if emails:
        print("\nüìß Emails found on discovered pages:")
        for e in emails:
            print("   -", e)
    if names:
        print("\nüßæ Candidate name/title tokens discovered:")
        for n in list(names)[:6]:
            print("   -", n)
    if snippets:
        print("\nüîé Snippet examples (first 200 chars):")
        for s in snippets[:3]:
            print("   >", s)
    print("\n‚ö†Ô∏è LEGAL DISCLAIMER: OSINT only. Do not misuse this tool.")
    print("="*70)

# -------------------------
# Save reports
# -------------------------
def save_json_report(filename: str, phone: str, phone_info: Dict[str,Any], results: List[Dict[str,Any]], external_carrier: Optional[Dict[str,Any]] = None):
    payload = {'phone': phone, 'phone_info': phone_info, 'external_carrier': external_carrier, 'results': results, 'generated_at': time.strftime("%Y-%m-%dT%H:%M:%SZ", time.gmtime())}
    try:
        with open(filename, "w", encoding="utf-8") as fh:
            json.dump(payload, fh, indent=2, ensure_ascii=False)
        log.info("Saved JSON report to %s", filename)
    except Exception as e:
        log.error("Failed to save JSON: %s", e)

def save_csv_report(filename: str, results: List[Dict[str,Any]]):
    try:
        with open(filename, "w", newline='', encoding="utf-8") as fh:
            writer = csv.DictWriter(fh, fieldnames=['platform','method','found','url','username','details','error'])
            writer.writeheader()
            for r in results:
                writer.writerow({
                    'platform': r.get('platform'),
                    'method': r.get('method'),
                    'found': r.get('found'),
                    'url': r.get('url'),
                    'username': r.get('username'),
                    'details': json.dumps(r.get('details')) if isinstance(r.get('details'), (dict, list)) else r.get('details'),
                    'error': r.get('error')
                })
        log.info("Saved CSV report to %s", filename)
    except Exception as e:
        log.error("Failed to save CSV: %s", e)

# -------------------------
# CLI
# -------------------------
def build_argparser():
    p = argparse.ArgumentParser(description="OSINT Social Media Finder (Upgraded)")
    p.add_argument("-n","--number", help="Phone number to search (E.164 or national)", required=False)
    p.add_argument("-c","--country", help="Default country code for parsing (e.g., IN, US)", default=None)
    p.add_argument("--numverify", help="Numverify API key (optional) to improve carrier lookup", default=None)
    p.add_argument("--proxies", help="Path to proxy file (one proxy per line)", default=None)
    p.add_argument("--json", help="Save JSON report to file", metavar="FILE")
    p.add_argument("--csv", help="Save CSV report to file", metavar="FILE")
    p.add_argument("--threads", help="Max concurrent thread count", type=int, default=8)
    p.add_argument("--delay", help="Delay between requests inside each search function", type=float, default=DEFAULT_DELAY)
    p.add_argument("--no-banner", help="Don't show banner", action="store_true")
    return p

def main():
    args = build_argparser().parse_args()
    if not args.no_banner:
        banner()
    phone_input = args.number
    if not phone_input:
        try:
            phone_input = input("Enter Phone Number: ").strip()
        except KeyboardInterrupt:
            print("Aborted.")
            sys.exit(0)
    country = args.country or None
    print("\n[1/3] Validating phone number...")
    info = parse_phone_number(phone_input, country)
    if not info.get('valid'):
        print("‚ùå Error:", info.get('error'))
        sys.exit(1)
    print(f"   ‚úÖ Valid: {info['international']}")
    print(f"   üìç Region: {info.get('region')}")
    print(f"   üì° Carrier (local DB): {info.get('carrier')}")
    # Load proxies
    proxies = None
    if args.proxies:
        proxies = load_proxies_from_file(args.proxies)
        if proxies:
            log.info("Loaded %d proxies", len(proxies))
        else:
            log.warning("No proxies loaded from %s", args.proxies)
    # External carrier lookup (optional)
    external_carrier = None
    if args.numverify:
        log.info("Querying external carrier lookup (numverify)...")
        external_carrier = lookup_carrier_numverify(info['e164'], args.numverify)
        if external_carrier:
            log.info("External carrier: %s", external_carrier.get('carrier'))
        else:
            log.warning("External lookup returned no data or failed.")
    print("\n[2/3] Scanning platforms (this may take a few minutes)...")
    results = comprehensive_social_search(info['e164'], proxies=proxies, delay=args.delay, max_threads=args.threads)
    print("\n[3/3] Generating report...")
    display_results(results, info, external_carrier=external_carrier)
    # Save
    if args.json:
        save_json_report(args.json, info['e164'], info, results, external_carrier)
        print(f"\nüìÅ Saved report: {args.json}")
    if args.csv:
        save_csv_report(args.csv, results)
        print(f"\nüìÅ Saved CSV: {args.csv}")
    # default save
    if not args.json and not args.csv:
        safe = info['e164'].replace('+','plus').replace(' ','_').replace('-','_')
        default = f"{safe}_report.json"
        save_json_report(default, info['e164'], info, results, external_carrier)
        print(f"\nüìÅ Saved report: {default}")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        print("\nInterrupted. Exiting.")
        sys.exit(0)
